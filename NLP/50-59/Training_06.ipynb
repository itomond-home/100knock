{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第6章: 機械学習\n",
    "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. データの入手・整形\n",
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "\n",
    "情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "\n",
    "抽出された事例をランダムに並び替える．\n",
    "\n",
    "抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．\n",
    "\n",
    "ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行数の確認\n",
    "!wc -l ./newsCorpora.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先頭10行の確認\n",
    "!head -10 ./newsCorpora.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
    "!sed -e 's/\"/'\\''/g' ./newsCorpora.csv > ./newsCorpora_re.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、pandasのデータフレームで読み込み、問題文の指示に従いデータを作成していきます。\n",
    "\n",
    "データの分割にはscikit-learnのtrain_test_splitを利用しています。その際stratifyオプションを利用すると、指定したカラムの構成比が分割後の各データで等しくなるように分割されます。\n",
    "\n",
    "ここでは、分類の目的変数であるCATEGORYを指定し、データごとに偏りが生じないようにしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データの読込\n",
    "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "# データの抽出\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
    "\n",
    "# データの分割\n",
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
    "\n",
    "# データの保存\n",
    "train.to_csv('./train.txt', sep='\\t', index=False)\n",
    "valid.to_csv('./valid.txt', sep='\\t', index=False)\n",
    "test.to_csv('./test.txt', sep='\\t', index=False)\n",
    "\n",
    "# 事例数の確認\n",
    "print('【学習データ】')\n",
    "print(train['CATEGORY'].value_counts())\n",
    "print('【検証データ】')\n",
    "print(valid['CATEGORY'].value_counts())\n",
    "print('【評価データ】')\n",
    "print(test['CATEGORY'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 51. 特徴量抽出\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ．\n",
    "\n",
    "なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def preprocessing(text):\n",
    "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  # 記号をスペースに置換\n",
    "  text = text.translate(table)\n",
    "  # 小文字化\n",
    "  text = text.lower()\n",
    "  # 数字列を0に置換\n",
    "  text = re.sub('[0-9]+', '0', text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの再結合\n",
    "df = pd.concat([train, valid, test], axis=0)\n",
    "# indexを振りなおす\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 前処理の実施\n",
    "df['TITLE'] = df['TITLE'].map(lambda x: preprocessing(x))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# データの分割\n",
    "train_valid = df[:len(train) + len(valid)]\n",
    "test = df[len(train) + len(valid):]\n",
    "\n",
    "# TfidfVectorizer\n",
    "# ngram_rangeでTF-IDFを計算する単語の長さを指定\n",
    "vec_tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2))\n",
    "\n",
    "# ベクトル化\n",
    "X_train_valid = vec_tfidf.fit_transform(train_valid['TITLE'])\n",
    "X_test = vec_tfidf.transform(test['TITLE'])\n",
    "\n",
    "# ベクトルをデータフレームに変換\n",
    "X_train_valid = pd.DataFrame(X_train_valid.toarray(), columns=vec_tfidf.get_feature_names_out())\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=vec_tfidf.get_feature_names_out())\n",
    "\n",
    "# データの分割\n",
    "X_train = X_train_valid[:len(train)]\n",
    "X_valid = X_train_valid[len(train):]\n",
    "\n",
    "# データの保存\n",
    "X_train.to_csv('./X_train.txt', sep='\\t', index=False)\n",
    "X_valid.to_csv('./X_valid.txt', sep='\\t', index=False)\n",
    "X_test.to_csv('./X_test.txt', sep='\\t', index=False)\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "100knock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
