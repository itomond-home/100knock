{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第5章: 係り受け解析\n",
    "日本語Wikipediaの「人工知能」に関する記事からテキスト部分を抜き出したファイルがai.ja.zipに収録されている．\n",
    "\n",
    "この文章をCaboChaやKNP等のツールを利用して係り受け解析を行い，その結果をai.ja.txt.parsedというファイルに保存せよ．\n",
    "\n",
    "このファイルを読み込み，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行数の確認\n",
    "!wc -l ./ai.ja.txt.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先頭15行の確認\n",
    "!head -15 ./ai.ja.txt.parsed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．\n",
    "\n",
    "このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．\n",
    "\n",
    "さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．\n",
    "\n",
    "※ メンバ変数とは、「通常 self.variable_name の形式で記述される」変数のこと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file = 'ai.ja.txt.parsed'\n",
    "\n",
    "class Morph:\n",
    "  def __init__(self, morph):\n",
    "    # 入力部分をタブで分割し、それぞれを格納\n",
    "    surface, attr = morph.split('\\t')\n",
    "    attr = attr.split(',')\n",
    "    self.surface = surface\n",
    "    self.base = attr[6]\n",
    "    self.pos = attr[0]\n",
    "    self.pos1 = attr[1]\n",
    "\n",
    "sentences = []\n",
    "morphs = []\n",
    "# ファイルを開く\n",
    "with open(parse_file, mode='r') as f:\n",
    "  # １行ずつ取り出して操作\n",
    "  for line in f:\n",
    "    # 取り込んだ最初が「*」なら飛ばす\n",
    "    if line[0] == '*':\n",
    "      continue\n",
    "    # 取り込んだ要素が「EOS」以外ならリストに追加する\n",
    "    elif line != 'EOS\\n':\n",
    "      morphs.append(Morph(line))\n",
    "    # 取り込んだ要素が「EOS」なら、文章の終わりなので、溜まったmorphリストをsentencesリストに追加する\n",
    "    else:\n",
    "      sentences.append(morphs)\n",
    "      morphs = []\n",
    "\n",
    "# 確認\n",
    "for m in sentences[2]:\n",
    "  print(vars(m))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．\n",
    "\n",
    "このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．\n",
    "\n",
    "さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk():\n",
    "  def __init__(self, morphs, dst):\n",
    "    self.morphs = morphs\n",
    "    self.dst = dst\n",
    "    self.srcs = []\n",
    "\n",
    "class Sentence():\n",
    "  def __init__(self, chunks):\n",
    "    self.chunks = chunks\n",
    "\n",
    "    # chunksを１行ずつ取り出す\n",
    "    for i, chunk in enumerate(self.chunks):\n",
    "      # chunksが持っているdst属性を参照する\n",
    "      if not chunk.dst != -1:\n",
    "        self.chunks[chunk.dst].srcs.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file = 'ai.ja.txt.parsed'\n",
    "\n",
    "sentences = [] # 各文章を加えていく\n",
    "chunks = []    # 各文節を加えていく\n",
    "morphs = []    # 各形態素を加えていく\n",
    "with open(parse_file, mode='r') as f:\n",
    "  dst = -1\n",
    "  for line in f:\n",
    "    if line[0] == '*':\n",
    "      # morphsに形態素が入っている（つまり、前の文節が終了した）ことを確認\n",
    "      if len(morphs) > 0:\n",
    "        chunks.append(Chunk(morphs, dst))\n",
    "        morphs = []\n",
    "      dst = int(line.split(' ')[2].rstrip('D'))\n",
    "    elif line != 'EOS\\n':\n",
    "      morphs.append(Morph(line))\n",
    "    else:\n",
    "      chunks.append(Chunk(morphs, dst))\n",
    "      sentences.append(Sentence(chunks))\n",
    "      morphs = []\n",
    "      chunks = []\n",
    "      dst = None\n",
    "\n",
    "# 確認\n",
    "for chunk in sentences[2].chunks:\n",
    "  print([morph.surface for morph in chunk.morphs], chunk.dst, chunk.srcs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences[2]\n",
    "for chunk in sentence.chunks:\n",
    "  if int(chunk.dst) != -1:\n",
    "    modifier = ''.join([morph.surface if morph.pos != '記号' else '' for morph in chunk.morphs])\n",
    "    modifiee = ''.join([morph.surface if morph.pos != '記号' else '' for morph in sentence.chunks[int(chunk.dst)].morphs])\n",
    "    print(modifier, modifiee, sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences[2]\n",
    "for chunk in sentence.chunks:\n",
    "  if int(chunk.dst) != -1:\n",
    "    modifier = ''.join([morph.surface if morph.pos != '記号' else '' for morph in chunk.morphs])\n",
    "    # 係る方の品詞を登録\n",
    "    modifier_pos = [morph.pos for morph in chunk.morphs]\n",
    "    modifiee = ''.join([morph.surface if morph.pos != '記号' else '' for morph in sentence.chunks[int(chunk.dst)].morphs])\n",
    "    # 係られる方の品詞を登録\n",
    "    modifiee_pos = [morph.pos for morph in sentence.chunks[int(chunk.dst)].morphs]\n",
    "\n",
    "    if '名詞' in modifier_pos and '動詞' in modifiee_pos:\n",
    "      print(modifier, modifiee, sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from IPython.display import Image,display_png\n",
    "from graphviz import Digraph\n",
    "\n",
    "sentence = sentences[7]\n",
    "edges = []\n",
    "for id, chunk in enumerate(sentence.chunks):\n",
    "  if int(chunk.dst) != -1:\n",
    "    modifier = ''.join([morph.surface if morph.pos != '記号' else '' for morph in chunk.morphs] + ['(' + str(id) + ')'])\n",
    "    modifiee = ''.join([morph.surface if morph.pos != '記号' else '' for morph in sentence.chunks[int(chunk.dst)].morphs] + ['(' + str(chunk.dst) + ')'])\n",
    "    edges.append([modifier, modifiee])\n",
    "n = pydot.Node('node')\n",
    "n.fontname = 'IPAGothic'\n",
    "g = pydot.graph_from_edges(edges, directed=True)\n",
    "g.add_node(n)\n",
    "g.write_png('./ans44.png')\n",
    "display_png(Image('./ans44.png'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ．\n",
    "\n",
    "ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ans45.txt', 'w') as f:\n",
    "  for sentence in sentences:\n",
    "    for chunk in sentence.chunks:\n",
    "      for morph in chunk.morphs:\n",
    "        # chunkの左から順番に動詞を探す\n",
    "        if morph.pos == '動詞':\n",
    "          cases = []\n",
    "          for src in chunk.srcs:\n",
    "            # 見つけた動詞の係り元chunkから助詞を探す\n",
    "            cases = cases + [morph.surface for morph in sentence.chunks[src].morphs if morph.pos == '助詞']\n",
    "          if len(cases) > 0: # casesの長さは助詞の長さ、0より大きいということは助詞があるということ\n",
    "            cases = sorted(list(set(cases)))\n",
    "            line = '{}\\t{}'.format(morph.base, ' '.join(cases))\n",
    "            print(line, file=f)\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認\n",
    "!cat ./ans45.txt | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ans46.txt', 'w') as f:\n",
    "  for sentence in sentences:\n",
    "    for chunk in sentence.chunks:\n",
    "      for morph in chunk.morphs:\n",
    "        # chunkの左から順番に動詞を探す\n",
    "        if morph.pos == '動詞':\n",
    "          cases = []\n",
    "          modi_chunks = []\n",
    "          for src in chunk.srcs:\n",
    "            # 見つけた動詞の係り元chunkから助詞を探す\n",
    "            case = [morph.surface for morph in sentence.chunks[src].morphs if morph.pos == '助詞']\n",
    "            if len(case) > 0: # 助詞を含んでいる場合は、助詞と項を取得\n",
    "              cases = cases + case\n",
    "              modi_chunks.append(''.join(morph.surface for morph in sentence.chunks[src].morphs if morph.pos != '記号'))\n",
    "          if len(cases) > 0: # casesの長さは助詞の長さ、0より大きいということは助詞があるということ\n",
    "            cases = sorted(list(set(cases)))\n",
    "            line = '{}\\t{}\\t{}'.format(morph.base, ' '.join(cases), ' '.join(modi_chunks))\n",
    "            print(line, file=f)\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認\n",
    "!cat ./ans46.txt | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "- コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ans47.txt', 'w') as f:\n",
    "  for sentence in sentences:\n",
    "    for chunk in sentence.chunks:\n",
    "      for morph in chunk.morphs:\n",
    "        if morph.pos == '動詞':\n",
    "          for i, src in enumerate(chunk.srcs):\n",
    "            # 見つけた動詞の係元が「サ変接続+を」かを確認\n",
    "            if len(sentence.chunks[src].morphs) == 2 and sentence.chunks[src].morphs[0].pos1 == 'サ変接続' and sentence.chunks[src].morphs[1].surface == 'を':\n",
    "              predicate = ''.join([sentence.chunks[src].morphs[0].surface, sentence.chunks[src].morphs[1].surface, morph.base])\n",
    "              cases = []\n",
    "              modi_chunks = []\n",
    "              # サ変接続を除いた、残りの係り元chunkから助詞を探す\n",
    "              for src_r in chunk.srcs[:i]+chunk.srcs[i+1:]:\n",
    "                case = [morph.surface for morph in sentence.chunks[src_r].morphs if morph.pos == '助詞']\n",
    "                if len(case) > 0:  # 助詞を含むchunkの場合は助詞と項を取得\n",
    "                  cases = cases + case\n",
    "                  modi_chunks.append(''.join(morph.surface for morph in sentence.chunks[src_r].morphs if morph.pos != '記号'))\n",
    "              # 助詞が1つ以上見つかった場合は重複除去後辞書順にソートし、項と合わせて出力\n",
    "              if len(cases) > 0:\n",
    "                cases = sorted(list(set(cases)))\n",
    "                line = '{}\\t{}\\t{}'.format(predicate, ' '.join(cases), ' '.join(modi_chunks))\n",
    "                print(line, file=f)\n",
    "              break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認\n",
    "!cat ./ans47.txt | cut -f 1 | sort | uniq -c | sort -nr | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "各文節は（表層形の）形態素列で表現する\n",
    "パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences[2]\n",
    "for chunk in sentence.chunks:\n",
    "  # chunkに名詞が含まれているかを判別\n",
    "  if '名詞' in [morph.pos for morph in chunk.morphs]:\n",
    "    path = [''.join(morph.surface for morph in chunk.morphs if morph.pos != '記号')]\n",
    "    # 名詞を含むchunkを先頭に、dstを根まで順に辿ってリストに追加、dst:依存関係のこと、-1になったらそこが根であることを表す\n",
    "    while chunk.dst != -1:\n",
    "      path.append(''.join(morph.surface for morph in sentence.chunks[chunk.dst].morphs if morph.pos != '記号'))\n",
    "      chunk = sentence.chunks[chunk.dst]\n",
    "    print(' -> '.join(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が i と j (i < j) のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "- 文節 i と j に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また、係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節 i から構文木の根に至る経路上に文節 j が存在する場合: 文節 i から文節 j のパスを表示\n",
    "- 上記以外で，文節 i と文節 j から構文木の根に至る経路上で共通の文節 k で交わる場合: 文節 i から文節 k に至る直前のパスと文節 j から文節 k に至る直前までのパス，文節 k の内容を” | “で連結して表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、\n",
    "\n",
    "- i -> a -> b -> j -> 根 であれば、 i -> a -> b -> j\n",
    "- i -> a -> k -> 根、j -> b -> k -> 根 であれば、i -> a | j -> b | k\n",
    "\n",
    "とし、i、j の名詞をそれぞれX、Yに変換して表示すればよいことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import re\n",
    "\n",
    "sentence = sentences[2]\n",
    "# 空のリスト、後に名詞句のインデックスを格納する\n",
    "nouns = []\n",
    "for i, chunk in enumerate(sentence.chunks):\n",
    "  # 名詞を含む文節を抽出\n",
    "  if '名詞' in [morph.pos for morph in chunk.morphs]:\n",
    "    nouns.append(i)\n",
    "for i, j in combinations(nouns, 2):\n",
    "  # i と j 、名刺から他の名刺への道のりを保存\n",
    "  path_i = []\n",
    "  path_j = []\n",
    "  while i != j:\n",
    "    if i < j:\n",
    "      path_i.append(i)\n",
    "      i = sentence.chunks[i].dst\n",
    "    else:\n",
    "      path_j.append(j)\n",
    "      j = sentence.chunks[j].dst\n",
    "\n",
    "  # 1つ目のケース\n",
    "  if len(path_j) == 0:\n",
    "    chunk_X = ''.join([morph.surface if morph.pos != '名詞' else 'X' for morph in sentence.chunks[path_i[0]].morphs])\n",
    "    chunk_Y = ''.join([morph.surface if morph.pos != '名詞' else 'Y' for morph in sentence.chunks[i].morphs])\n",
    "    # 名刺の部分を X と Y に置換\n",
    "    chunk_X = re.sub('X+', 'X', chunk_X)\n",
    "    chunk_Y = re.sub('Y+', 'Y', chunk_Y)\n",
    "    path_XtoY = [chunk_X] + [''.join(morph.surface for morph in sentence.chunks[n].morphs) for n in path_i[1:]] + [chunk_Y]\n",
    "    print(' -> '.join(path_XtoY))\n",
    "\n",
    "  # 2つ目のケース\n",
    "  else:\n",
    "    chunk_X = ''.join([morph.surface if morph.pos != '名詞' else 'X' for morph in sentence.chunks[path_i[0]].morphs])\n",
    "    chunk_Y = ''.join([morph.surface if morph.pos != '名詞' else 'Y' for morph in sentence.chunks[path_j[0]].morphs])\n",
    "    chunk_k = ''.join([morph.surface for morph in sentence.chunks[i].morphs])\n",
    "    chunk_X = re.sub('X+', 'X', chunk_X)\n",
    "    chunk_Y = re.sub('Y+', 'Y', chunk_Y)\n",
    "    path_X = [chunk_X] + [''.join(morph.surface for morph in sentence.chunks[n].morphs) for n in path_i[1:]]\n",
    "    path_Y = [chunk_Y] + [''.join(morph.surface for morph in sentence.chunks[n].morphs) for n in path_j[1:]]\n",
    "    print(' | '.join([' -> '.join(path_X), ' -> '.join(path_Y), chunk_k]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "100knock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
